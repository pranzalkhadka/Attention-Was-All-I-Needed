{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the length of input sequence\n",
    "sequence_length = 4\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# This is the dimension of the input vector for every single word or token \n",
    "input_dimension = 512\n",
    "\n",
    "# This is the output of attention block for every single word\n",
    "d_model = 512\n",
    "\n",
    "# This is the randomly sampled input data since no positional encoding is used here\n",
    "x = torch.randn((batch_size, sequence_length, input_dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1536])\n"
     ]
    }
   ],
   "source": [
    "# Creating the KEY, QUERY and VALUE vectors\n",
    "qkv_layer = nn.Linear(input_dimension , 3 * d_model)\n",
    "qkv = qkv_layer(x)\n",
    "print(qkv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if number of heads is 8, it means simply 8 different sets of learnable parameters operate in parallel all of which get same input but produce different context aware vectors. Each head may learn to attend to different parts of the input, capturing various patterns or information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8, 192])\n"
     ]
    }
   ],
   "source": [
    "# The multi head attention will have 8 heads. That means 8 different outputs will be generated from the attention block\n",
    "# 8 different linear blocks will run simultaneously that do not share wieghts\n",
    "\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "print(qkv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 192])\n"
     ]
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3)\n",
    "print(qkv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY Vector : torch.Size([1, 8, 4, 64])\n",
      "KEY Vector : torch.Size([1, 8, 4, 64])\n",
      "VALUE Vector : torch.Size([1, 8, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "print(f\"QUERY Vector : {q.shape}\")\n",
    "print(f\"KEY Vector : {k.shape}\")\n",
    "print(f\"VALUE Vector : {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Performing dot product between query and key vectors and then scaling it with sqaure root of dimension of KEY vector\n",
    "\n",
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "print(scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.full(scaled.size() , float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "# mask for input to a single head\n",
    "print(mask[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled += mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = F.softmax(scaled, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5102, 0.4898, 0.0000, 0.0000],\n",
       "        [0.4090, 0.3680, 0.2230, 0.0000],\n",
       "        [0.2990, 0.2103, 0.3047, 0.1860]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see after applying sofmax, all negative infinity gets converted to zeroes. So, in decoder, prediction only happens by looking at previous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the dot product between the output and VALUE vectors to get the final values which are now more context aware\n",
    "\n",
    "values = torch.matmul(attention, v)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape of tensors, we can see for a single batch, we have 8 heads for input sequence length 4 and 64 size vectors for every single word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine all the attention heads since at last we are gonna be needing the same shape as input vector. This concatenation can be said that all attetnion heads will communicate with each other to produce final context aware vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "values = values.reshape(batch_size, sequence_length, num_heads * head_dim)\n",
    "print(values.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = linear_layer(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0051,  0.0204, -0.1722,  ...,  0.0487,  0.1819,  0.1497],\n",
       "         [ 0.0182, -0.4404,  0.0071,  ...,  0.0053,  0.2802, -0.6434],\n",
       "         [-0.1348,  0.1406, -0.1483,  ...,  0.1058, -0.4081,  0.1045],\n",
       "         [ 0.1432, -0.0169,  0.0990,  ...,  0.1788,  0.0016,  0.2969]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final vector which is now more context aware than what it originally was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
